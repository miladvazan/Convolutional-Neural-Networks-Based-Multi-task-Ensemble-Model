{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-w3jok3qgxo",
        "outputId": "511b5c1d-9f46-4321-bfa6-c0c1ba3494fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhnZqIb8E0mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b11637-5a9b-48dd-b836-6fa12c7b36a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "your_module = drive.CreateFile({'id':'174Ws0LnaEQdK7UmOKh-8GJwftO18Mfej'})\n",
        "your_module.GetContentFile('metric.py')\n",
        "your_module1 = drive.CreateFile({'id':'11ssSCDodnZ6_jXcHLZ8AqYHbg7g7K8Be'})\n",
        "your_module1.GetContentFile('threshold.py')\n",
        "your_module2 = drive.CreateFile({'id':'1JpuTOGEKMq9iZcO6Tp4ET9icd5EG0P0D'})\n",
        "your_module2.GetContentFile('cat_list.py')\n",
        "your_module2 = drive.CreateFile({'id':'1gsMrGL0BfVASQICf-eesMRelmCgMLTYs'})\n",
        "your_module2.GetContentFile('cat_list_show.py')\n",
        "from metric import *\n",
        "from threshold import *\n",
        "from cat_list import *\n",
        "from cat_list_show import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPYld0Nm_Zjn"
      },
      "source": [
        "def jacard(y_test, predictions):\n",
        "    accuracy = 0.0\n",
        "\n",
        "    for i in range(y_test.shape[0]):\n",
        "        intersection = 0.0\n",
        "        union = 0.0\n",
        "        for j in range(y_test.shape[1]):\n",
        "            if int(y_test[i,j]) == 1 or int(predictions[i,j]) == 1:\n",
        "                union += 1\n",
        "            if int(y_test[i,j]) == 1 and int(predictions[i,j]) == 1:\n",
        "                intersection += 1\n",
        "            if int(y_test[i,j]) == 2 or int(predictions[i,j]) == 2:\n",
        "                union += 1\n",
        "            if int(y_test[i,j]) == 2 and int(predictions[i,j]) == 2:\n",
        "                intersection += 1\n",
        "            \n",
        "        if union != 0:\n",
        "            accuracy = accuracy + float(intersection/union)\n",
        "\n",
        "    accuracy = float(accuracy/y_test.shape[0])\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HoFkqWFMfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b6d610-b2bf-441c-8b88-cfcb35fdfb52"
      },
      "source": [
        "!pip install tensorflow-determinism"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-determinism\n",
            "  Downloading tensorflow-determinism-0.3.0.tar.gz (12 kB)\n",
            "Building wheels for collected packages: tensorflow-determinism\n",
            "  Building wheel for tensorflow-determinism (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-determinism: filename=tensorflow_determinism-0.3.0-py3-none-any.whl size=9158 sha256=cb64da250695b1e27b0ce60f7d7b6a74dd120d412e7aac196c721f73cdb2b1bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/be/33/2b27e81e5d40b4bfb7c103ac6c6c5e81fdbcf40d2af5078529\n",
            "Successfully built tensorflow-determinism\n",
            "Installing collected packages: tensorflow-determinism\n",
            "Successfully installed tensorflow-determinism-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EQHORIeE0nS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc36df7-c9d4-469f-a339-f1a43b271d22"
      },
      "source": [
        "#تکرار پذیری مدل\n",
        "import os\n",
        "sd = 1024\n",
        "os.environ['PYTHONHASHSEED']=str(sd)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import random as rn\n",
        "random.seed(sd)\n",
        "np.random.seed(sd)\n",
        "rn.seed(sd)\n",
        "from keras import backend as K\n",
        "\n",
        "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
        "#فراخوانی کتابخانه ها\n",
        "import pandas as pd\n",
        "from tensorflow.python.keras.models import Model,Sequential\n",
        "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from sklearn.metrics import classification_report\n",
        "from time import time\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
        "\n",
        "#فراخوانی داده های آموزشی و آزمایشی\n",
        "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
        "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
        "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
        "                              'سبک','فیلم',\n",
        "                             'فیلم_نامه',\n",
        "                             'محتوا',\n",
        "                            'موضوع','کارگردان',\n",
        "                            ]]\n",
        "\n",
        "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
        "                              'سبک','فیلم',\n",
        "                             'فیلم_نامه',\n",
        "                             'محتوا',\n",
        "                            'موضوع','کارگردان',\n",
        "                            ]]\n",
        "category_list=['بازیگر','بازیگری','داستان',\n",
        "                              'سبک','فیلم',\n",
        "                             'فیلم_نامه',\n",
        "                             'محتوا',\n",
        "                            'موضوع','کارگردان',\n",
        "                            ]\n",
        "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
        "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
        "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
        "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
        "            ]\n",
        "#پیش پردازش\n",
        "import re\n",
        "def preprocess_text(sentence):\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'@\\S+', '', sentence)\n",
        "    sentence = re.sub(r'!\\S+', '', sentence)\n",
        "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
        "    sentence = re.sub(r'[.]', ' ', sentence)\n",
        "    sentence = re.sub(r'[/]', '', sentence)\n",
        "    sentence = re.sub(r'[،]', ' ', sentence)\n",
        "    sentence = re.sub(r'[؛]', '', sentence)\n",
        "    sentence = sentence.split()\n",
        "    sentence =[word for word in sentence if word not in stop_words]\n",
        "    sentence = ' '.join(sentence)\n",
        "    return sentence\n",
        "def ReplacetwoMore(s):\n",
        "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
        "    return pattern.sub(r\"\\1\", s)\n",
        "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
        "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
        "X = []\n",
        "sentences = list(dataset[\"clean_text\"])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))    \n",
        "X_t = []\n",
        "sentences2 = list(test_file[\"clean_text\"])\n",
        "for sen in sentences2:\n",
        "    X_t.append(preprocess_text(sen))\n",
        "np.random.seed(sd)\n",
        "\n",
        "y = S_comments_labels.values\n",
        "y_test1=S_test_comments_labels.values\n",
        "target =y\n",
        "data = X\n",
        "target1 = y_test1\n",
        "data1 = X_t\n",
        "x_train, x_test = data, data1\n",
        "y_train, y_test = target, target1\n",
        "np.random.seed(sd)\n",
        "print(x_train[136])\n",
        "print(y_train[136])\n",
        "y1_train = dataset[[\"بازیگر\"]].values\n",
        "y1_test =  test_file[[\"بازیگر\"]].values\n",
        "y2_train = dataset[[\"بازیگری\"]].values\n",
        "y2_test =  test_file[[\"بازیگری\"]].values\n",
        "y3_train = dataset[[\"داستان\"]].values\n",
        "y3_test =  test_file[[\"داستان\"]].values\n",
        "\n",
        "y4_train = dataset[[\"سبک\"]].values\n",
        "y4_test =  test_file[[\"سبک\"]].values\n",
        "y5_train = dataset[[\"فیلم\"]].values\n",
        "y5_test =  test_file[[\"فیلم\"]].values\n",
        "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
        "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
        "\n",
        "\n",
        "y7_train = dataset[[\"محتوا\"]].values\n",
        "y7_test =  test_file[[\"محتوا\"]].values\n",
        "\n",
        "y8_train = dataset[[\"موضوع\"]].values\n",
        "y8_test =  test_file[[\"موضوع\"]].values\n",
        "y9_train = dataset[[\"کارگردان\"]].values\n",
        "y9_test =  test_file[[\"کارگردان\"]].values\n",
        "\n",
        "#شمارش تعداد کلمات منحصر به فرد\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "all_words=' '.join(data)\n",
        "all_words=word_tokenize(all_words)\n",
        "dist=FreqDist(all_words)\n",
        "num_unique_word=len(dist)\n",
        "print ('number unique word:',num_unique_word)\n",
        "#شمارش بزرگترین طول متن\n",
        "r_len=[]\n",
        "for text in data:\n",
        "    word=word_tokenize(text)\n",
        "    l=len(word)\n",
        "    r_len.append(l)   \n",
        "MAX_REVIEW_LEN=np.max(r_len)\n",
        "print('max len:',MAX_REVIEW_LEN)\n",
        "np.random.seed(sd)\n",
        "num_words = num_unique_word\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "np.random.seed(sd)\n",
        "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
        "print(x_train[100])\n",
        "print(x_train_tokens[100])\n",
        "max_tokens = MAX_REVIEW_LEN\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
        "np.random.seed(sd)\n",
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "def tokens_to_string(tokens):\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "print('train shape',x_train_pad.shape)\n",
        "print('test shape',x_test_pad.shape)\n",
        "np.random.seed(sd)\n",
        "#طراحی مدل\n",
        "embedding_size = 300\n",
        "input_1 = Input(shape=(max_tokens,))\n",
        "x=Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    name='embedding_layer',\n",
        "            embeddings_initializer=initializer)(input_1)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "x=Conv1D(256,kernel_size=3,padding='same',activation='relu',strides=1 ,kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=GlobalMaxPooling1D()(x)\n",
        "\n",
        "x=Dense(200, activation='relu',\n",
        "        kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
        "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
        "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
        "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
        "output5 = Dense(3, activation='softmax')(x)\n",
        "output6 = Dense(3, activation='softmax')(x)\n",
        "output7 = Dense(3, activation='softmax')(x)\n",
        "output8 = Dense(3, activation='softmax')(x)\n",
        "output9 = Dense(3, activation='softmax')(x)\n",
        "model1 = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
        "                                      ])\n",
        "\n",
        "optimizer =Nadam(learning_rate=1e-3)\n",
        "model1.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,)\n",
        "model1.summary()\n",
        "np.random.seed(sd)\n",
        "history=model1.fit(x_train_pad, \n",
        "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
        "                                  y5_train,y6_train,y7_train,\n",
        "                                  y8_train,y9_train]\n",
        "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
        "\n",
        "predicted = model1.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted, axis=-1) \n",
        "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
        "p=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "یک کلام عالی همه چیز فیلم نامه فیلم برداری کارگردان بازیگران موضوع متفاوت فوقالعاده بود یک کلام\n",
            "[1 0 0 0 0 1 0 1 1]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "number unique word: 4730\n",
            "max len: 103\n",
            "عالی فیلم دارای سر ته مشخص جمع جوری داشت منون کلیه عوامل فیلم\n",
            "[6, 1, 705, 62, 153, 354, 787, 642, 23, 322, 1280, 123, 1]\n",
            "train shape (2000, 103)\n",
            "test shape (200, 103)\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 103)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer (Embedding)     (None, 103, 300)     1419000     input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 103, 300)     0           embedding_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 103, 256)     230400      dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 103, 256)     1024        conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_9 (GlobalM (None, 256)          0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 200)          51200       global_max_pooling1d_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 200)          0           dense_54[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bazigar (Dense)                 (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bazigari (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dastan (Dense)                  (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dialog (Dense)                  (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_56 (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_57 (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_59 (Dense)                (None, 3)            603         dropout_20[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,707,051\n",
            "Trainable params: 1,706,539\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "Train on 2000 samples\n",
            "Epoch 1/20\n",
            "2000/2000 [==============================] - 1s 666us/sample - loss: 6.0658 - bazigar_loss: 0.6260 - bazigari_loss: 0.7651 - dastan_loss: 0.4986 - dialog_loss: 0.5254 - dense_55_loss: 1.0251 - dense_56_loss: 0.6681 - dense_57_loss: 0.7295 - dense_58_loss: 0.5402 - dense_59_loss: 0.6416\n",
            "Epoch 2/20\n",
            "2000/2000 [==============================] - 0s 164us/sample - loss: 3.9948 - bazigar_loss: 0.4474 - bazigari_loss: 0.5783 - dastan_loss: 0.3112 - dialog_loss: 0.3005 - dense_55_loss: 1.0326 - dense_56_loss: 0.3664 - dense_57_loss: 0.3310 - dense_58_loss: 0.2996 - dense_59_loss: 0.3249\n",
            "Epoch 3/20\n",
            "2000/2000 [==============================] - 0s 127us/sample - loss: 3.6849 - bazigar_loss: 0.4342 - bazigari_loss: 0.5412 - dastan_loss: 0.2951 - dialog_loss: 0.2884 - dense_55_loss: 0.8785 - dense_56_loss: 0.3240 - dense_57_loss: 0.3193 - dense_58_loss: 0.2917 - dense_59_loss: 0.3096\n",
            "Epoch 4/20\n",
            "2000/2000 [==============================] - 0s 123us/sample - loss: 3.2653 - bazigar_loss: 0.4185 - bazigari_loss: 0.5081 - dastan_loss: 0.2786 - dialog_loss: 0.2741 - dense_55_loss: 0.6882 - dense_56_loss: 0.2346 - dense_57_loss: 0.2846 - dense_58_loss: 0.2824 - dense_59_loss: 0.2915\n",
            "Epoch 5/20\n",
            "2000/2000 [==============================] - 0s 118us/sample - loss: 2.7688 - bazigar_loss: 0.4026 - bazigari_loss: 0.4223 - dastan_loss: 0.2464 - dialog_loss: 0.2638 - dense_55_loss: 0.5362 - dense_56_loss: 0.1320 - dense_57_loss: 0.2514 - dense_58_loss: 0.2444 - dense_59_loss: 0.2650\n",
            "Epoch 6/20\n",
            "2000/2000 [==============================] - 0s 119us/sample - loss: 2.1580 - bazigar_loss: 0.3364 - bazigari_loss: 0.2891 - dastan_loss: 0.1769 - dialog_loss: 0.2300 - dense_55_loss: 0.4316 - dense_56_loss: 0.0909 - dense_57_loss: 0.2051 - dense_58_loss: 0.1661 - dense_59_loss: 0.2247\n",
            "Epoch 7/20\n",
            "2000/2000 [==============================] - 0s 116us/sample - loss: 1.7230 - bazigar_loss: 0.2972 - bazigari_loss: 0.2320 - dastan_loss: 0.1239 - dialog_loss: 0.1980 - dense_55_loss: 0.3586 - dense_56_loss: 0.0662 - dense_57_loss: 0.1519 - dense_58_loss: 0.1269 - dense_59_loss: 0.1627\n",
            "Epoch 8/20\n",
            "2000/2000 [==============================] - 0s 120us/sample - loss: 1.3818 - bazigar_loss: 0.2484 - bazigari_loss: 0.1919 - dastan_loss: 0.0938 - dialog_loss: 0.1448 - dense_55_loss: 0.3035 - dense_56_loss: 0.0570 - dense_57_loss: 0.1311 - dense_58_loss: 0.1023 - dense_59_loss: 0.1038\n",
            "Epoch 9/20\n",
            "2000/2000 [==============================] - 0s 121us/sample - loss: 1.1322 - bazigar_loss: 0.2043 - bazigari_loss: 0.1597 - dastan_loss: 0.0767 - dialog_loss: 0.1058 - dense_55_loss: 0.2677 - dense_56_loss: 0.0473 - dense_57_loss: 0.1061 - dense_58_loss: 0.0813 - dense_59_loss: 0.0774\n",
            "Epoch 10/20\n",
            "2000/2000 [==============================] - 0s 117us/sample - loss: 0.8823 - bazigar_loss: 0.1687 - bazigari_loss: 0.1185 - dastan_loss: 0.0684 - dialog_loss: 0.0760 - dense_55_loss: 0.1995 - dense_56_loss: 0.0328 - dense_57_loss: 0.0794 - dense_58_loss: 0.0730 - dense_59_loss: 0.0624\n",
            "Epoch 11/20\n",
            "2000/2000 [==============================] - 0s 117us/sample - loss: 0.7180 - bazigar_loss: 0.1397 - bazigari_loss: 0.0973 - dastan_loss: 0.0462 - dialog_loss: 0.0601 - dense_55_loss: 0.1658 - dense_56_loss: 0.0339 - dense_57_loss: 0.0659 - dense_58_loss: 0.0561 - dense_59_loss: 0.0500\n",
            "Epoch 12/20\n",
            "2000/2000 [==============================] - 0s 119us/sample - loss: 0.6086 - bazigar_loss: 0.1065 - bazigari_loss: 0.0999 - dastan_loss: 0.0367 - dialog_loss: 0.0510 - dense_55_loss: 0.1341 - dense_56_loss: 0.0273 - dense_57_loss: 0.0602 - dense_58_loss: 0.0511 - dense_59_loss: 0.0383\n",
            "Epoch 13/20\n",
            "2000/2000 [==============================] - 0s 118us/sample - loss: 0.4823 - bazigar_loss: 0.0896 - bazigari_loss: 0.0525 - dastan_loss: 0.0373 - dialog_loss: 0.0377 - dense_55_loss: 0.1116 - dense_56_loss: 0.0239 - dense_57_loss: 0.0439 - dense_58_loss: 0.0463 - dense_59_loss: 0.0371\n",
            "Epoch 14/20\n",
            "2000/2000 [==============================] - 0s 120us/sample - loss: 0.3662 - bazigar_loss: 0.0562 - bazigari_loss: 0.0492 - dastan_loss: 0.0257 - dialog_loss: 0.0234 - dense_55_loss: 0.0873 - dense_56_loss: 0.0261 - dense_57_loss: 0.0355 - dense_58_loss: 0.0332 - dense_59_loss: 0.0278\n",
            "Epoch 15/20\n",
            "2000/2000 [==============================] - 0s 121us/sample - loss: 0.3457 - bazigar_loss: 0.0667 - bazigari_loss: 0.0428 - dastan_loss: 0.0228 - dialog_loss: 0.0254 - dense_55_loss: 0.0810 - dense_56_loss: 0.0207 - dense_57_loss: 0.0285 - dense_58_loss: 0.0314 - dense_59_loss: 0.0245\n",
            "Epoch 16/20\n",
            "2000/2000 [==============================] - 0s 121us/sample - loss: 0.2531 - bazigar_loss: 0.0366 - bazigari_loss: 0.0333 - dastan_loss: 0.0218 - dialog_loss: 0.0191 - dense_55_loss: 0.0577 - dense_56_loss: 0.0149 - dense_57_loss: 0.0210 - dense_58_loss: 0.0292 - dense_59_loss: 0.0189\n",
            "Epoch 17/20\n",
            "2000/2000 [==============================] - 0s 117us/sample - loss: 0.2075 - bazigar_loss: 0.0306 - bazigari_loss: 0.0278 - dastan_loss: 0.0131 - dialog_loss: 0.0163 - dense_55_loss: 0.0446 - dense_56_loss: 0.0156 - dense_57_loss: 0.0206 - dense_58_loss: 0.0187 - dense_59_loss: 0.0193\n",
            "Epoch 18/20\n",
            "2000/2000 [==============================] - 0s 119us/sample - loss: 0.1757 - bazigar_loss: 0.0247 - bazigari_loss: 0.0231 - dastan_loss: 0.0134 - dialog_loss: 0.0159 - dense_55_loss: 0.0362 - dense_56_loss: 0.0130 - dense_57_loss: 0.0207 - dense_58_loss: 0.0169 - dense_59_loss: 0.0116\n",
            "Epoch 19/20\n",
            "2000/2000 [==============================] - 0s 116us/sample - loss: 0.1481 - bazigar_loss: 0.0195 - bazigari_loss: 0.0181 - dastan_loss: 0.0105 - dialog_loss: 0.0112 - dense_55_loss: 0.0331 - dense_56_loss: 0.0109 - dense_57_loss: 0.0130 - dense_58_loss: 0.0195 - dense_59_loss: 0.0119\n",
            "Epoch 20/20\n",
            "2000/2000 [==============================] - 0s 124us/sample - loss: 0.1434 - bazigar_loss: 0.0175 - bazigari_loss: 0.0212 - dastan_loss: 0.0124 - dialog_loss: 0.0113 - dense_55_loss: 0.0318 - dense_56_loss: 0.0122 - dense_57_loss: 0.0129 - dense_58_loss: 0.0125 - dense_59_loss: 0.0111\n",
            "0.04333333333333334\n",
            "0.7928333333333335\n",
            "0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 200\n",
        "input_1 = Input(shape=(max_tokens,))\n",
        "x=Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    name='embedding_layer',\n",
        "            embeddings_initializer=initializer)(input_1)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "x=Conv1D(256,kernel_size=3,padding='same',activation='relu',strides=1 ,kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=GlobalMaxPooling1D()(x)\n",
        "\n",
        "x=Dense(200, activation='relu',\n",
        "        kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
        "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
        "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
        "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
        "output5 = Dense(3, activation='softmax')(x)\n",
        "output6 = Dense(3, activation='softmax')(x)\n",
        "output7 = Dense(3, activation='softmax')(x)\n",
        "output8 = Dense(3, activation='softmax')(x)\n",
        "output9 = Dense(3, activation='softmax')(x)\n",
        "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
        "                                      ])\n",
        "\n",
        "optimizer =Nadam(learning_rate=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,)\n",
        "model.summary()\n",
        "np.random.seed(sd)\n",
        "history=model.fit(x_train_pad, \n",
        "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
        "                                  y5_train,y6_train,y7_train,\n",
        "                                  y8_train,y9_train]\n",
        "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
        "\n",
        "predicted2 = model.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted2, axis=-1) \n",
        "p2=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p2[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umwuCEPXtA0J",
        "outputId": "a77c6b27-8e6f-4ce4-aeb1-4806f78c08b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 103)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer (Embedding)     (None, 103, 200)     946000      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 103, 200)     0           embedding_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 103, 256)     153600      dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 103, 256)     1024        conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 256)          0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 200)          51200       global_max_pooling1d_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 200)          0           dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bazigar (Dense)                 (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bazigari (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dastan (Dense)                  (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dialog (Dense)                  (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 3)            603         dropout_14[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,157,251\n",
            "Trainable params: 1,156,739\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "Train on 2000 samples\n",
            "Epoch 1/20\n",
            "2000/2000 [==============================] - 1s 476us/sample - loss: 5.9603 - bazigar_loss: 0.7196 - bazigari_loss: 0.7778 - dastan_loss: 0.5619 - dialog_loss: 0.5770 - dense_37_loss: 0.9850 - dense_38_loss: 0.5759 - dense_39_loss: 0.6339 - dense_40_loss: 0.5433 - dense_41_loss: 0.5428\n",
            "Epoch 2/20\n",
            "2000/2000 [==============================] - 0s 99us/sample - loss: 3.9879 - bazigar_loss: 0.4331 - bazigari_loss: 0.5656 - dastan_loss: 0.3125 - dialog_loss: 0.3090 - dense_37_loss: 1.0255 - dense_38_loss: 0.3730 - dense_39_loss: 0.3274 - dense_40_loss: 0.3113 - dense_41_loss: 0.3296\n",
            "Epoch 3/20\n",
            "2000/2000 [==============================] - 0s 95us/sample - loss: 3.7194 - bazigar_loss: 0.4287 - bazigari_loss: 0.5239 - dastan_loss: 0.2939 - dialog_loss: 0.2922 - dense_37_loss: 0.9138 - dense_38_loss: 0.3404 - dense_39_loss: 0.3143 - dense_40_loss: 0.2982 - dense_41_loss: 0.3126\n",
            "Epoch 4/20\n",
            "2000/2000 [==============================] - 0s 101us/sample - loss: 3.2784 - bazigar_loss: 0.4074 - bazigari_loss: 0.4134 - dastan_loss: 0.2770 - dialog_loss: 0.2874 - dense_37_loss: 0.7380 - dense_38_loss: 0.2715 - dense_39_loss: 0.2893 - dense_40_loss: 0.2887 - dense_41_loss: 0.3030\n",
            "Epoch 5/20\n",
            "2000/2000 [==============================] - 0s 98us/sample - loss: 2.7694 - bazigar_loss: 0.3879 - bazigari_loss: 0.2999 - dastan_loss: 0.2404 - dialog_loss: 0.2755 - dense_37_loss: 0.5762 - dense_38_loss: 0.1686 - dense_39_loss: 0.2727 - dense_40_loss: 0.2721 - dense_41_loss: 0.2732\n",
            "Epoch 6/20\n",
            "2000/2000 [==============================] - 0s 97us/sample - loss: 2.3237 - bazigar_loss: 0.3554 - bazigari_loss: 0.2467 - dastan_loss: 0.1775 - dialog_loss: 0.2593 - dense_37_loss: 0.4593 - dense_38_loss: 0.1184 - dense_39_loss: 0.2280 - dense_40_loss: 0.2415 - dense_41_loss: 0.2352\n",
            "Epoch 7/20\n",
            "2000/2000 [==============================] - 0s 99us/sample - loss: 1.9323 - bazigar_loss: 0.3158 - bazigari_loss: 0.2092 - dastan_loss: 0.1373 - dialog_loss: 0.2462 - dense_37_loss: 0.3760 - dense_38_loss: 0.0900 - dense_39_loss: 0.1853 - dense_40_loss: 0.1781 - dense_41_loss: 0.1905\n",
            "Epoch 8/20\n",
            "2000/2000 [==============================] - 0s 93us/sample - loss: 1.6068 - bazigar_loss: 0.2900 - bazigari_loss: 0.1801 - dastan_loss: 0.1073 - dialog_loss: 0.2327 - dense_37_loss: 0.3117 - dense_38_loss: 0.0706 - dense_39_loss: 0.1472 - dense_40_loss: 0.1222 - dense_41_loss: 0.1416\n",
            "Epoch 9/20\n",
            "2000/2000 [==============================] - 0s 96us/sample - loss: 1.3524 - bazigar_loss: 0.2506 - bazigari_loss: 0.1569 - dastan_loss: 0.0894 - dialog_loss: 0.1864 - dense_37_loss: 0.2748 - dense_38_loss: 0.0562 - dense_39_loss: 0.1210 - dense_40_loss: 0.1062 - dense_41_loss: 0.1055\n",
            "Epoch 10/20\n",
            "2000/2000 [==============================] - 0s 96us/sample - loss: 1.0949 - bazigar_loss: 0.2130 - bazigari_loss: 0.1304 - dastan_loss: 0.0735 - dialog_loss: 0.1484 - dense_37_loss: 0.2223 - dense_38_loss: 0.0464 - dense_39_loss: 0.0971 - dense_40_loss: 0.0858 - dense_41_loss: 0.0732\n",
            "Epoch 11/20\n",
            "2000/2000 [==============================] - 0s 99us/sample - loss: 0.9098 - bazigar_loss: 0.1828 - bazigari_loss: 0.1056 - dastan_loss: 0.0648 - dialog_loss: 0.1081 - dense_37_loss: 0.1829 - dense_38_loss: 0.0399 - dense_39_loss: 0.0932 - dense_40_loss: 0.0718 - dense_41_loss: 0.0569\n",
            "Epoch 12/20\n",
            "2000/2000 [==============================] - 0s 99us/sample - loss: 0.7357 - bazigar_loss: 0.1442 - bazigari_loss: 0.0940 - dastan_loss: 0.0533 - dialog_loss: 0.0712 - dense_37_loss: 0.1566 - dense_38_loss: 0.0351 - dense_39_loss: 0.0708 - dense_40_loss: 0.0589 - dense_41_loss: 0.0487\n",
            "Epoch 13/20\n",
            "2000/2000 [==============================] - 0s 97us/sample - loss: 0.5866 - bazigar_loss: 0.1089 - bazigari_loss: 0.0682 - dastan_loss: 0.0403 - dialog_loss: 0.0572 - dense_37_loss: 0.1293 - dense_38_loss: 0.0278 - dense_39_loss: 0.0591 - dense_40_loss: 0.0525 - dense_41_loss: 0.0413\n",
            "Epoch 14/20\n",
            "2000/2000 [==============================] - 0s 98us/sample - loss: 0.4912 - bazigar_loss: 0.0975 - bazigari_loss: 0.0613 - dastan_loss: 0.0346 - dialog_loss: 0.0471 - dense_37_loss: 0.1016 - dense_38_loss: 0.0230 - dense_39_loss: 0.0489 - dense_40_loss: 0.0459 - dense_41_loss: 0.0281\n",
            "Epoch 15/20\n",
            "2000/2000 [==============================] - 0s 100us/sample - loss: 0.3965 - bazigar_loss: 0.0672 - bazigari_loss: 0.0484 - dastan_loss: 0.0280 - dialog_loss: 0.0368 - dense_37_loss: 0.0953 - dense_38_loss: 0.0198 - dense_39_loss: 0.0365 - dense_40_loss: 0.0374 - dense_41_loss: 0.0257\n",
            "Epoch 16/20\n",
            "2000/2000 [==============================] - 0s 96us/sample - loss: 0.3700 - bazigar_loss: 0.0654 - bazigari_loss: 0.0472 - dastan_loss: 0.0250 - dialog_loss: 0.0353 - dense_37_loss: 0.0849 - dense_38_loss: 0.0224 - dense_39_loss: 0.0312 - dense_40_loss: 0.0329 - dense_41_loss: 0.0225\n",
            "Epoch 17/20\n",
            "2000/2000 [==============================] - 0s 95us/sample - loss: 0.2826 - bazigar_loss: 0.0503 - bazigari_loss: 0.0373 - dastan_loss: 0.0213 - dialog_loss: 0.0258 - dense_37_loss: 0.0574 - dense_38_loss: 0.0164 - dense_39_loss: 0.0278 - dense_40_loss: 0.0276 - dense_41_loss: 0.0174\n",
            "Epoch 18/20\n",
            "2000/2000 [==============================] - 0s 96us/sample - loss: 0.2291 - bazigar_loss: 0.0383 - bazigari_loss: 0.0298 - dastan_loss: 0.0189 - dialog_loss: 0.0172 - dense_37_loss: 0.0502 - dense_38_loss: 0.0141 - dense_39_loss: 0.0224 - dense_40_loss: 0.0243 - dense_41_loss: 0.0136\n",
            "Epoch 19/20\n",
            "2000/2000 [==============================] - 0s 96us/sample - loss: 0.2071 - bazigar_loss: 0.0305 - bazigari_loss: 0.0266 - dastan_loss: 0.0160 - dialog_loss: 0.0193 - dense_37_loss: 0.0484 - dense_38_loss: 0.0187 - dense_39_loss: 0.0175 - dense_40_loss: 0.0195 - dense_41_loss: 0.0102\n",
            "Epoch 20/20\n",
            "2000/2000 [==============================] - 0s 98us/sample - loss: 0.1709 - bazigar_loss: 0.0296 - bazigari_loss: 0.0202 - dastan_loss: 0.0169 - dialog_loss: 0.0092 - dense_37_loss: 0.0371 - dense_38_loss: 0.0133 - dense_39_loss: 0.0126 - dense_40_loss: 0.0184 - dense_41_loss: 0.0134\n",
            "0.042777777777777776\n",
            "0.7907500000000001\n",
            "0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100\n",
        "input_1 = Input(shape=(max_tokens,))\n",
        "x=Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    name='embedding_layer',\n",
        "            embeddings_initializer=initializer)(input_1)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "x=Conv1D(256,kernel_size=3,padding='same',activation='relu',strides=1 ,kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=GlobalMaxPooling1D()(x)\n",
        "\n",
        "x=Dense(200, activation='relu',\n",
        "        kernel_initializer=initializer,use_bias=False)(x)\n",
        "x=Dropout(0.2,seed=sd)(x)\n",
        "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
        "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
        "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
        "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
        "output5 = Dense(3, activation='softmax')(x)\n",
        "output6 = Dense(3, activation='softmax')(x)\n",
        "output7 = Dense(3, activation='softmax')(x)\n",
        "output8 = Dense(3, activation='softmax')(x)\n",
        "output9 = Dense(3, activation='softmax')(x)\n",
        "model3 = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
        "                                      ])\n",
        "\n",
        "optimizer =Nadam(learning_rate=1e-3)\n",
        "model3.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,)\n",
        "model3.summary()\n",
        "np.random.seed(sd)\n",
        "history=model3.fit(x_train_pad, \n",
        "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
        "                                  y5_train,y6_train,y7_train,\n",
        "                                  y8_train,y9_train]\n",
        "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
        "\n",
        "predicted3 = model3.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted3, axis=-1) \n",
        "p3=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p3[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5IIfo7kyieS",
        "outputId": "516687fd-5d7a-42c9-b5ba-35f6b1b1f998"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 103)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer (Embedding)     (None, 103, 100)     473000      input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 103, 100)     0           embedding_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 103, 256)     76800       dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 103, 256)     1024        conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_12 (Global (None, 256)          0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 200)          51200       global_max_pooling1d_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 200)          0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bazigar (Dense)                 (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bazigari (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dastan (Dense)                  (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dialog (Dense)                  (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_76 (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_77 (Dense)                (None, 3)            603         dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 607,451\n",
            "Trainable params: 606,939\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "Train on 2000 samples\n",
            "Epoch 1/20\n",
            "2000/2000 [==============================] - 1s 644us/sample - loss: 6.1365 - bazigar_loss: 0.6658 - bazigari_loss: 0.7761 - dastan_loss: 0.5861 - dialog_loss: 0.5994 - dense_73_loss: 1.0227 - dense_74_loss: 0.6989 - dense_75_loss: 0.5128 - dense_76_loss: 0.6953 - dense_77_loss: 0.5344\n",
            "Epoch 2/20\n",
            "2000/2000 [==============================] - 0s 124us/sample - loss: 4.0310 - bazigar_loss: 0.4498 - bazigari_loss: 0.5707 - dastan_loss: 0.3129 - dialog_loss: 0.2995 - dense_73_loss: 1.0544 - dense_74_loss: 0.3840 - dense_75_loss: 0.3269 - dense_76_loss: 0.3038 - dense_77_loss: 0.3285\n",
            "Epoch 3/20\n",
            "2000/2000 [==============================] - 0s 88us/sample - loss: 3.8244 - bazigar_loss: 0.4327 - bazigari_loss: 0.5466 - dastan_loss: 0.3013 - dialog_loss: 0.2892 - dense_73_loss: 0.9953 - dense_74_loss: 0.3593 - dense_75_loss: 0.3085 - dense_76_loss: 0.2904 - dense_77_loss: 0.3028\n",
            "Epoch 4/20\n",
            "2000/2000 [==============================] - 0s 83us/sample - loss: 3.6754 - bazigar_loss: 0.4290 - bazigari_loss: 0.5176 - dastan_loss: 0.2975 - dialog_loss: 0.2900 - dense_73_loss: 0.9016 - dense_74_loss: 0.3525 - dense_75_loss: 0.3024 - dense_76_loss: 0.2854 - dense_77_loss: 0.2998\n",
            "Epoch 5/20\n",
            "2000/2000 [==============================] - 0s 104us/sample - loss: 3.3830 - bazigar_loss: 0.4147 - bazigari_loss: 0.4602 - dastan_loss: 0.2882 - dialog_loss: 0.2838 - dense_73_loss: 0.7499 - dense_74_loss: 0.3226 - dense_75_loss: 0.2893 - dense_76_loss: 0.2847 - dense_77_loss: 0.2890\n",
            "Epoch 6/20\n",
            "2000/2000 [==============================] - 0s 97us/sample - loss: 2.9796 - bazigar_loss: 0.3915 - bazigari_loss: 0.3428 - dastan_loss: 0.2863 - dialog_loss: 0.2786 - dense_73_loss: 0.6075 - dense_74_loss: 0.2569 - dense_75_loss: 0.2703 - dense_76_loss: 0.2769 - dense_77_loss: 0.2689\n",
            "Epoch 7/20\n",
            "2000/2000 [==============================] - 0s 89us/sample - loss: 2.5655 - bazigar_loss: 0.3695 - bazigari_loss: 0.2822 - dastan_loss: 0.2691 - dialog_loss: 0.2785 - dense_73_loss: 0.4697 - dense_74_loss: 0.1646 - dense_75_loss: 0.2311 - dense_76_loss: 0.2530 - dense_77_loss: 0.2461\n",
            "Epoch 8/20\n",
            "2000/2000 [==============================] - 0s 100us/sample - loss: 2.2390 - bazigar_loss: 0.3363 - bazigari_loss: 0.2483 - dastan_loss: 0.2464 - dialog_loss: 0.2717 - dense_73_loss: 0.4018 - dense_74_loss: 0.1072 - dense_75_loss: 0.1881 - dense_76_loss: 0.2210 - dense_77_loss: 0.2176\n",
            "Epoch 9/20\n",
            "2000/2000 [==============================] - 0s 117us/sample - loss: 1.9643 - bazigar_loss: 0.3182 - bazigari_loss: 0.2204 - dastan_loss: 0.2204 - dialog_loss: 0.2536 - dense_73_loss: 0.3442 - dense_74_loss: 0.0804 - dense_75_loss: 0.1546 - dense_76_loss: 0.1768 - dense_77_loss: 0.1923\n",
            "Epoch 10/20\n",
            "2000/2000 [==============================] - 0s 103us/sample - loss: 1.7393 - bazigar_loss: 0.3050 - bazigari_loss: 0.1884 - dastan_loss: 0.1813 - dialog_loss: 0.2397 - dense_73_loss: 0.2971 - dense_74_loss: 0.0667 - dense_75_loss: 0.1350 - dense_76_loss: 0.1472 - dense_77_loss: 0.1770\n",
            "Epoch 11/20\n",
            "2000/2000 [==============================] - 0s 112us/sample - loss: 1.5732 - bazigar_loss: 0.2792 - bazigari_loss: 0.1796 - dastan_loss: 0.1581 - dialog_loss: 0.2296 - dense_73_loss: 0.2646 - dense_74_loss: 0.0555 - dense_75_loss: 0.1172 - dense_76_loss: 0.1260 - dense_77_loss: 0.1617\n",
            "Epoch 12/20\n",
            "2000/2000 [==============================] - 0s 106us/sample - loss: 1.4086 - bazigar_loss: 0.2603 - bazigari_loss: 0.1586 - dastan_loss: 0.1175 - dialog_loss: 0.2230 - dense_73_loss: 0.2306 - dense_74_loss: 0.0582 - dense_75_loss: 0.0979 - dense_76_loss: 0.1095 - dense_77_loss: 0.1502\n",
            "Epoch 13/20\n",
            "2000/2000 [==============================] - 0s 94us/sample - loss: 1.2233 - bazigar_loss: 0.2324 - bazigari_loss: 0.1461 - dastan_loss: 0.0906 - dialog_loss: 0.2037 - dense_73_loss: 0.1912 - dense_74_loss: 0.0484 - dense_75_loss: 0.0830 - dense_76_loss: 0.0914 - dense_77_loss: 0.1350\n",
            "Epoch 14/20\n",
            "2000/2000 [==============================] - 0s 94us/sample - loss: 1.0982 - bazigar_loss: 0.2151 - bazigari_loss: 0.1311 - dastan_loss: 0.0777 - dialog_loss: 0.1871 - dense_73_loss: 0.1788 - dense_74_loss: 0.0411 - dense_75_loss: 0.0717 - dense_76_loss: 0.0862 - dense_77_loss: 0.1055\n",
            "Epoch 15/20\n",
            "2000/2000 [==============================] - 0s 91us/sample - loss: 0.9238 - bazigar_loss: 0.1931 - bazigari_loss: 0.1164 - dastan_loss: 0.0590 - dialog_loss: 0.1589 - dense_73_loss: 0.1424 - dense_74_loss: 0.0358 - dense_75_loss: 0.0569 - dense_76_loss: 0.0739 - dense_77_loss: 0.0846\n",
            "Epoch 16/20\n",
            "2000/2000 [==============================] - 0s 95us/sample - loss: 0.8167 - bazigar_loss: 0.1668 - bazigari_loss: 0.0978 - dastan_loss: 0.0590 - dialog_loss: 0.1369 - dense_73_loss: 0.1241 - dense_74_loss: 0.0376 - dense_75_loss: 0.0610 - dense_76_loss: 0.0644 - dense_77_loss: 0.0658\n",
            "Epoch 17/20\n",
            "2000/2000 [==============================] - 0s 92us/sample - loss: 0.6762 - bazigar_loss: 0.1308 - bazigari_loss: 0.0767 - dastan_loss: 0.0492 - dialog_loss: 0.1109 - dense_73_loss: 0.1183 - dense_74_loss: 0.0314 - dense_75_loss: 0.0450 - dense_76_loss: 0.0627 - dense_77_loss: 0.0498\n",
            "Epoch 18/20\n",
            "2000/2000 [==============================] - 0s 101us/sample - loss: 0.5692 - bazigar_loss: 0.1025 - bazigari_loss: 0.0715 - dastan_loss: 0.0377 - dialog_loss: 0.0827 - dense_73_loss: 0.1021 - dense_74_loss: 0.0272 - dense_75_loss: 0.0460 - dense_76_loss: 0.0547 - dense_77_loss: 0.0434\n",
            "Epoch 19/20\n",
            "2000/2000 [==============================] - 0s 105us/sample - loss: 0.4978 - bazigar_loss: 0.0977 - bazigari_loss: 0.0707 - dastan_loss: 0.0327 - dialog_loss: 0.0684 - dense_73_loss: 0.0886 - dense_74_loss: 0.0252 - dense_75_loss: 0.0359 - dense_76_loss: 0.0428 - dense_77_loss: 0.0342\n",
            "Epoch 20/20\n",
            "2000/2000 [==============================] - 0s 94us/sample - loss: 0.4288 - bazigar_loss: 0.0846 - bazigari_loss: 0.0489 - dastan_loss: 0.0326 - dialog_loss: 0.0488 - dense_73_loss: 0.0788 - dense_74_loss: 0.0232 - dense_75_loss: 0.0319 - dense_76_loss: 0.0480 - dense_77_loss: 0.0309\n",
            "0.04666666666666667\n",
            "0.7767499999999999\n",
            "0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model1.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted, axis=-1) \n",
        "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
        "p1=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p1[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjJNLmWfuEvZ",
        "outputId": "485611c6-314a-486e-808f-9859fec7b362"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04333333333333334\n",
            "0.7928333333333335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted, axis=-1) \n",
        "p=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CgFVRlatgdL",
        "outputId": "fe08eca5-405a-47c7-acc9-523594b47781"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04055555555555555\n",
            "0.8033333333333336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted3 = model3.predict(x_test_pad)\n",
        "pred_class = np.argmax(predicted3, axis=-1) \n",
        "p3=pred_class.T\n",
        "hm=[]\n",
        "ac=[]\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], p3[i]))\n",
        "print(np.mean(hm))\n",
        "print (jacard(y_test, p3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia54s0RrytRo",
        "outputId": "4cd2d0cb-4509-407e-ed99-7feefc6845b1"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04666666666666667\n",
            "0.7767499999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats as s\n",
        "from statistics import mode\n",
        "final_pred = []\n",
        "for i in range(0,len(x_test_pad)):\n",
        "  x = s.mode([p1[i], p[i], p3[i]])\n",
        "  final_pred.append(x[0])"
      ],
      "metadata": {
        "id": "R51u4ji2uH5B"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hm=[]\n",
        "for i in range(len(y_test)):\n",
        "  hm.append(hamming_loss(y_test[i], final_pred[i][0]))\n",
        "print(np.mean(hm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsrFgxXLqWNz",
        "outputId": "521980a1-6c5f-4a9c-a618-de0275d72cba"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03944444444444445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "ac=[]\n",
        "for i in range(len(y_test)):\n",
        "  ac.append(jaccard_score(y_test[i], final_pred[i][0],average='macro'))\n",
        "print(np.mean(ac))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-YQtWr817y4",
        "outputId": "b496d42c-e646-4d46-ee0a-00b1f7db1a35"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8764831349206349\n"
          ]
        }
      ]
    }
  ]
}